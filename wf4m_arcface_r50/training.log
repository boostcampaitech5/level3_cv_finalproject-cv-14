Training: 2023-07-17 05:12:45,218-rank_id: 0
Training: 2023-07-17 05:12:51,266-: margin_list              [1.0, 0.0, 0.4]
Training: 2023-07-17 05:12:51,266-: network                  r50
Training: 2023-07-17 05:12:51,266-: resume                   False
Training: 2023-07-17 05:12:51,266-: save_all_states          False
Training: 2023-07-17 05:12:51,266-: output                   wf4m_arcface_r50
Training: 2023-07-17 05:12:51,266-: embedding_size           512
Training: 2023-07-17 05:12:51,266-: sample_rate              1.0
Training: 2023-07-17 05:12:51,266-: interclass_filtering_threshold0
Training: 2023-07-17 05:12:51,266-: fp16                     True
Training: 2023-07-17 05:12:51,266-: batch_size               128
Training: 2023-07-17 05:12:51,266-: optimizer                sgd
Training: 2023-07-17 05:12:51,266-: lr                       0.1
Training: 2023-07-17 05:12:51,266-: momentum                 0.9
Training: 2023-07-17 05:12:51,266-: weight_decay             0.0005
Training: 2023-07-17 05:12:51,266-: verbose                  2000
Training: 2023-07-17 05:12:51,267-: frequent                 10
Training: 2023-07-17 05:12:51,267-: dali                     False
Training: 2023-07-17 05:12:51,267-: dali_aug                 False
Training: 2023-07-17 05:12:51,267-: gradient_acc             1
Training: 2023-07-17 05:12:51,267-: seed                     2048
Training: 2023-07-17 05:12:51,267-: num_workers              2
Training: 2023-07-17 05:12:51,267-: wandb_key                XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Training: 2023-07-17 05:12:51,267-: suffix_run_name          None
Training: 2023-07-17 05:12:51,267-: using_wandb              False
Training: 2023-07-17 05:12:51,267-: wandb_entity             entity
Training: 2023-07-17 05:12:51,267-: wandb_project            project
Training: 2023-07-17 05:12:51,267-: wandb_log_all            True
Training: 2023-07-17 05:12:51,267-: save_artifacts           False
Training: 2023-07-17 05:12:51,267-: wandb_resume             False
Training: 2023-07-17 05:12:51,267-: rec                      /opt/ml/project/data/faces_webface_112x112
Training: 2023-07-17 05:12:51,267-: num_classes              10571
Training: 2023-07-17 05:12:51,267-: num_image                494149
Training: 2023-07-17 05:12:51,267-: num_epoch                27
Training: 2023-07-17 05:12:51,267-: warmup_epoch             3
Training: 2023-07-17 05:12:51,267-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-07-17 05:12:51,268-: total_batch_size         128
Training: 2023-07-17 05:12:51,268-: warmup_step              11580
Training: 2023-07-17 05:12:51,268-: total_step               104220
Training: 2023-07-17 05:13:49,060-Reducer buckets have been rebuilt in this iteration.
Training: 2023-07-17 05:13:52,068-Speed 765.14 samples/sec   Loss 38.8143   LearningRate 0.000173   Epoch: 0   Global Step: 20   Fp16 Grad Scale: 16384   Required: 17 hours
Training: 2023-07-17 05:13:53,742-Speed 765.32 samples/sec   Loss 38.7707   LearningRate 0.000259   Epoch: 0   Global Step: 30   Fp16 Grad Scale: 16384   Required: 13 hours
Training: 2023-07-17 05:13:55,413-Speed 766.28 samples/sec   Loss 38.5737   LearningRate 0.000345   Epoch: 0   Global Step: 40   Fp16 Grad Scale: 8192   Required: 11 hours
Training: 2023-07-17 05:13:57,085-Speed 765.64 samples/sec   Loss 38.6475   LearningRate 0.000432   Epoch: 0   Global Step: 50   Fp16 Grad Scale: 8192   Required: 10 hours
Training: 2023-07-17 05:13:58,759-Speed 764.91 samples/sec   Loss 38.6898   LearningRate 0.000518   Epoch: 0   Global Step: 60   Fp16 Grad Scale: 8192   Required: 9 hours
Training: 2023-07-17 05:14:00,426-Speed 768.30 samples/sec   Loss 38.7876   LearningRate 0.000604   Epoch: 0   Global Step: 70   Fp16 Grad Scale: 8192   Required: 9 hours
Training: 2023-07-17 05:14:02,112-Speed 759.51 samples/sec   Loss 38.5906   LearningRate 0.000691   Epoch: 0   Global Step: 80   Fp16 Grad Scale: 8192   Required: 8 hours
Training: 2023-07-17 05:14:03,784-Speed 766.17 samples/sec   Loss 38.5161   LearningRate 0.000777   Epoch: 0   Global Step: 90   Fp16 Grad Scale: 8192   Required: 8 hours
Training: 2023-07-17 05:14:05,459-Speed 764.38 samples/sec   Loss 38.3987   LearningRate 0.000864   Epoch: 0   Global Step: 100   Fp16 Grad Scale: 8192   Required: 7 hours
Training: 2023-07-17 05:14:07,135-Speed 764.36 samples/sec   Loss 38.3697   LearningRate 0.000950   Epoch: 0   Global Step: 110   Fp16 Grad Scale: 8192   Required: 7 hours
Training: 2023-07-17 05:14:08,807-Speed 765.95 samples/sec   Loss 38.2630   LearningRate 0.001036   Epoch: 0   Global Step: 120   Fp16 Grad Scale: 8192   Required: 7 hours
Training: 2023-07-17 05:14:10,473-Speed 768.30 samples/sec   Loss 38.2242   LearningRate 0.001123   Epoch: 0   Global Step: 130   Fp16 Grad Scale: 8192   Required: 7 hours
Training: 2023-07-17 05:14:12,151-Speed 763.31 samples/sec   Loss 38.0255   LearningRate 0.001209   Epoch: 0   Global Step: 140   Fp16 Grad Scale: 16384   Required: 7 hours
Training: 2023-07-17 05:14:13,825-Speed 765.10 samples/sec   Loss 37.9903   LearningRate 0.001295   Epoch: 0   Global Step: 150   Fp16 Grad Scale: 16384   Required: 7 hours
Training: 2023-07-17 05:14:15,505-Speed 762.30 samples/sec   Loss 37.7471   LearningRate 0.001382   Epoch: 0   Global Step: 160   Fp16 Grad Scale: 16384   Required: 6 hours
Training: 2023-07-17 05:14:17,182-Speed 763.48 samples/sec   Loss 37.5983   LearningRate 0.001468   Epoch: 0   Global Step: 170   Fp16 Grad Scale: 16384   Required: 6 hours
Training: 2023-07-17 05:14:18,847-Speed 769.29 samples/sec   Loss 37.5723   LearningRate 0.001554   Epoch: 0   Global Step: 180   Fp16 Grad Scale: 16384   Required: 6 hours
Training: 2023-07-17 05:14:20,525-Speed 763.19 samples/sec   Loss 37.6061   LearningRate 0.001641   Epoch: 0   Global Step: 190   Fp16 Grad Scale: 16384   Required: 6 hours
Training: 2023-07-17 05:27:32,566-rank_id: 0
Training: 2023-07-17 05:27:36,672-: margin_list              [1.0, 0.0, 0.4]
Training: 2023-07-17 05:27:36,672-: network                  mobile
Training: 2023-07-17 05:27:36,672-: resume                   False
Training: 2023-07-17 05:27:36,672-: save_all_states          False
Training: 2023-07-17 05:27:36,672-: output                   wf4m_arcface_r50
Training: 2023-07-17 05:27:36,672-: embedding_size           512
Training: 2023-07-17 05:27:36,672-: sample_rate              1.0
Training: 2023-07-17 05:27:36,672-: interclass_filtering_threshold0
Training: 2023-07-17 05:27:36,672-: fp16                     True
Training: 2023-07-17 05:27:36,672-: batch_size               128
Training: 2023-07-17 05:27:36,673-: optimizer                sgd
Training: 2023-07-17 05:27:36,673-: lr                       0.1
Training: 2023-07-17 05:27:36,673-: momentum                 0.9
Training: 2023-07-17 05:27:36,673-: weight_decay             0.0005
Training: 2023-07-17 05:27:36,673-: verbose                  2000
Training: 2023-07-17 05:27:36,673-: frequent                 10
Training: 2023-07-17 05:27:36,673-: dali                     False
Training: 2023-07-17 05:27:36,673-: dali_aug                 False
Training: 2023-07-17 05:27:36,673-: gradient_acc             1
Training: 2023-07-17 05:27:36,673-: seed                     2048
Training: 2023-07-17 05:27:36,673-: num_workers              2
Training: 2023-07-17 05:27:36,673-: wandb_key                XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Training: 2023-07-17 05:27:36,673-: suffix_run_name          None
Training: 2023-07-17 05:27:36,673-: using_wandb              False
Training: 2023-07-17 05:27:36,673-: wandb_entity             entity
Training: 2023-07-17 05:27:36,673-: wandb_project            project
Training: 2023-07-17 05:27:36,673-: wandb_log_all            True
Training: 2023-07-17 05:27:36,673-: save_artifacts           False
Training: 2023-07-17 05:27:36,673-: wandb_resume             False
Training: 2023-07-17 05:27:36,673-: rec                      /opt/ml/project/data/faces_webface_112x112
Training: 2023-07-17 05:27:36,673-: num_classes              10571
Training: 2023-07-17 05:27:36,673-: num_image                494149
Training: 2023-07-17 05:27:36,674-: num_epoch                27
Training: 2023-07-17 05:27:36,674-: warmup_epoch             3
Training: 2023-07-17 05:27:36,674-: val_targets              ['lfw', 'cfp_fp', 'agedb_30']
Training: 2023-07-17 05:27:36,674-: total_batch_size         128
Training: 2023-07-17 05:27:36,674-: warmup_step              11580
Training: 2023-07-17 05:27:36,674-: total_step               104220
